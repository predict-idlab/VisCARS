{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from rdflib import Graph\n",
    "from rdflib.namespace import RDF, SOSA\n",
    "import tqdm\n",
    "from time import sleep\n",
    "\n",
    "from viscars.dao import DAO, ContentRecommenderDAO, VisualizationRecommenderDAO\n",
    "import viscars.evaluation.evaluators as evaluators\n",
    "from viscars.evaluation.metrics.factory import MetricFactory, MetricType\n",
    "from viscars.namespace import DASHB\n",
    "import viscars.recommenders as recommenders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PerformanceEvaluator:\n",
    "\n",
    "    def __init__(self, dao: DAO):\n",
    "        self.visualizations = list(set(dao.graph.subjects(RDF.type, DASHB.Visualization)))\n",
    "        self.properties = list(set(dao.graph.subjects(RDF.type, SOSA.ObservableProperty)))\n",
    "        self.dao = dao\n",
    "\n",
    "    def run(self, evaluator, recommenders, metrics, iter=5, sleep_=10, output_path_=None):\n",
    "        metric_factory = MetricFactory()\n",
    "\n",
    "        parsed_metrics = []\n",
    "        for metric in metrics:\n",
    "            m_split = metric.split('@')\n",
    "            m_type = m_split[0]\n",
    "            n = int(m_split[1]) if len(m_split) >= 2 else None\n",
    "\n",
    "            metric_ = metric_factory.get(MetricType.reverse_lookup(m_type), n)\n",
    "            parsed_metrics.append(metric_)\n",
    "\n",
    "        results = {}\n",
    "        for id_, cls in tqdm(recommenders.items()):\n",
    "            recommender_ = cls(self.dao)\n",
    "            evaluator_ = evaluator(self.dao, recommender_, metrics=parsed_metrics)\n",
    "\n",
    "            scores = {}\n",
    "            for _ in range(iter):\n",
    "                sleep(sleep_)\n",
    "\n",
    "                scores_ = evaluator_.evaluate()\n",
    "                for metric_, score_ in scores_['result'].items():\n",
    "                    if metric_ in scores.keys():\n",
    "                        scores[metric_] += score_\n",
    "                    else:\n",
    "                        scores[metric_] = score_\n",
    "\n",
    "            scores = {k: v / iter for k, v in scores.items()}\n",
    "            results[id_] = scores\n",
    "            if output_path_ is not None:\n",
    "                with open(os.path.join(output_path_, 'performance.json'), 'w') as output_f:\n",
    "                    output_f.write(json.dumps(results))\n",
    "\n",
    "        return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_ = Graph()\n",
    "graph_.parse('../data/protego/protego_ddashboard.ttl')\n",
    "graph_.parse('../data/protego/protego_zplus.ttl')\n",
    "graph_.parse('../data/protego/visualizations.ttl')\n",
    "\n",
    "iter_ = 3  # Number of iterations for the CV\n",
    "sleep_ = 0  # Optional timeout between iterations (in seconds)\n",
    "\n",
    "models_ = {\n",
    "    'PPR': recommenders.PPR,\n",
    "    'MF': recommenders.MF,\n",
    "    'CF': recommenders.CF,\n",
    "    'PR': recommenders.PR,\n",
    "    'RANDOM': recommenders.RandomRank\n",
    "}\n",
    "\n",
    "dao_ = VisualizationRecommenderDAO(graph_)\n",
    "evaluator = PerformanceEvaluator(dao_)\n",
    "metrics = ['f1@1']\n",
    "# vis_results_kfold = evaluator.run(evaluators.KFoldCrossValidation, models_, metrics, iter=iter_, sleep_=sleep_, output_path_=output_path)\n",
    "vis_results_loocv = evaluator.run(evaluators.LeaveOneOutCrossValidation, models_, metrics, iter=iter_, sleep_=sleep_)\n",
    "\n",
    "dao_ = ContentRecommenderDAO(graph_)\n",
    "evaluator = PerformanceEvaluator(dao_)\n",
    "metrics = ['f1', 'ndcg']\n",
    "# content_results_kfold = evaluator.run(evaluators.KFoldCrossValidation, models_, metrics, iter=iter_, sleep_=sleep_, output_path_=output_path)\n",
    "# content_results_loocv = evaluator.run(evaluators.LeaveOneOutCrossValidation, models_, metrics, iter=iter_, sleep_=sleep_, output_path_=output_path)\n",
    "\n",
    "results = {\n",
    "    'visualization_rec': {\n",
    "        # 'kfold': vis_results_kfold,\n",
    "        'loocv': vis_results_loocv\n",
    "    },\n",
    "    'content_rec': {\n",
    "        # 'kfold': content_results_kfold,\n",
    "        # 'loocv': content_results_loocv\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
